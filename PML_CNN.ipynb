{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buex9BUIhTKk"
      },
      "source": [
        "## Image Classification MNIST Dataset With CNN 1 Convolution, and Jittering, Normalization Images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ],
      "metadata": {
        "id": "atjTBW7SrXEb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldfgddLO6omi"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from torchvision import datasets,transforms\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jittering and Normalization Image Dataset"
      ],
      "metadata": {
        "id": "zrg4N4USrkWB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6SuG4Q57xIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6431ec4-2442-4b25-d6ee-b997b17b1e9a"
      },
      "source": [
        "set_latih = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,                         \n",
        "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,)),transforms.ColorJitter()]), \n",
        "    download = True,            \n",
        ")\n",
        "set_uji = datasets.MNIST(\n",
        "    root = 'data', \n",
        "    train = False, \n",
        "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,)),transforms.ColorJitter()]),\n",
        "    download = True,\n",
        ")\n",
        "latih = torch.utils.data.DataLoader(set_latih, batch_size=64, shuffle=True)\n",
        "uji = torch.utils.data.DataLoader(set_uji, batch_size=64, shuffle=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 76051831.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 112058921.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 21780002.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 20202045.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set learning rate, momentum, and make device with GPU"
      ],
      "metadata": {
        "id": "ZLLlglN8sUok"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_3pDrc6Uyid"
      },
      "source": [
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Training Model with 1 CNN and 1 Linear Layer"
      ],
      "metadata": {
        "id": "8dtIzGr0sccI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyb5e7hIEEzh"
      },
      "source": [
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.convolutaional_neural_network_layers = nn.Sequential(\n",
        "                nn.Conv2d(in_channels=1, out_channels=5, kernel_size=5, padding=1, stride=1), \n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.linear_layers = nn.Sequential(\n",
        "                nn.Linear(in_features=5*13*13, out_features=20),          \n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm1d(num_features=20),\n",
        "                nn.Dropout(p=0.2),\n",
        "                nn.Linear(in_features=20, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convolutaional_neural_network_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Model from Class Network and Then Using SGD Optimizer and Cross Entropy Loss"
      ],
      "metadata": {
        "id": "OQym3_eKspyv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5aOBH_dC75o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a12498c-5e3a-4dad-f328-da3ca59fffbe"
      },
      "source": [
        "model = Network()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "model.to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (convolutaional_neural_network_layers): Sequential(\n",
              "    (0): Conv2d(1, 5, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (linear_layers): Sequential(\n",
              "    (0): Linear(in_features=845, out_features=20, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): Dropout(p=0.2, inplace=False)\n",
              "    (4): Linear(in_features=20, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process Training and Testing From Model"
      ],
      "metadata": {
        "id": "8X6SQcQYtDBf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWAMzmQqDYSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9eee1b-4b9f-4e14-a409-5688b9f708c1"
      },
      "source": [
        "train_loss, val_loss = [], []\n",
        "accuracy_total_train, accuracy_total_val = [], []\n",
        "max_epoch = 10\n",
        "for epoch in range(max_epoch):\n",
        "   \n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "    model.train()\n",
        "    total = 0\n",
        "\n",
        "    for idx, (image, label) in enumerate(latih):\n",
        "        gambar, label = image.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(gambar)\n",
        "        loss = criteria(pred, label)\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "                \n",
        "    accuracy_train = total / len(set_latih)\n",
        "    accuracy_total_train.append(accuracy_train)\n",
        "    total_train_loss = total_train_loss / (idx + 1)\n",
        "    train_loss.append(total_train_loss)\n",
        "    model.eval()\n",
        "    total = 0\n",
        "\n",
        "    for idx, (image, label) in enumerate(uji):\n",
        "        gambar, label = image.cuda(), label.cuda()\n",
        "        pred = model(gambar)\n",
        "        loss = criteria(pred, label)\n",
        "        total_val_loss += loss.item()\n",
        "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "        for i, p in enumerate(pred):\n",
        "            if label[i] == torch.max(p.data, 0)[1]:\n",
        "                total = total + 1\n",
        "\n",
        "    accuracy_val = total / len(set_uji)\n",
        "    accuracy_total_val.append(accuracy_val)\n",
        "    total_val_loss = total_val_loss / (idx + 1)\n",
        "    val_loss.append(total_val_loss)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "      print(\"Epoch: {}  \".format(epoch),\n",
        "            \"Training loss: {:.2f}  \".format(total_train_loss),\n",
        "            \"Testing loss: {:.2f}  \".format(total_val_loss),\n",
        "            \"Train accuracy: {:.2f}  \".format(accuracy_train),\n",
        "            \"Test accuracy: {:.2f}  \".format(accuracy_val))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0   Training loss: 0.44   Testing loss: 0.14   Train accuracy: 0.90   Test accuracy: 0.97  \n",
            "Epoch: 2   Training loss: 0.13   Testing loss: 0.07   Train accuracy: 0.97   Test accuracy: 0.98  \n",
            "Epoch: 4   Training loss: 0.10   Testing loss: 0.07   Train accuracy: 0.97   Test accuracy: 0.98  \n",
            "Epoch: 6   Training loss: 0.09   Testing loss: 0.06   Train accuracy: 0.97   Test accuracy: 0.98  \n",
            "Epoch: 8   Training loss: 0.08   Testing loss: 0.05   Train accuracy: 0.98   Test accuracy: 0.98  \n"
          ]
        }
      ]
    }
  ]
}